Introduction:

Sound architects need to get to immense assortments of sound eﬀects for their ﬁlm and video creations. Sound eﬀects suppliers depend on text-recovery methods to oﬀer their assortments. Annotation of sound substance was done physically, which was a very difficult task. Programmed explanation strategies, ordinarily ﬁne-tuned to reduced areas, for example, instruments or diminished sound effects scientific classifications, are not adult enough for naming with great detail any conceivable sound. An overall sound acknowledgment instrument would require a) A scientific categorization that addresses the world and, b) Thousands of classiﬁers, each worked in distinctive minimal details. In this assignment a sound annotator is created to merge two separate audio files which are 1. Voice commentary and 2. Audio of a football match. The task is to mix the commentary to the specific part of the football match audio so that one can hear both the match and the commentary, and the user would be eventually able to export the merged audio file. All this feature would be possible with a user-friendly interface which supports interactions with the software functions like playing, pausing, stopping, merging, and exporting the merged audio.

Background:

Any sound, whatever it very well might be, is brought about by something vibrating. As such, by something which is moving to and from, either in a customary way or in an arbitrary way, about the position it involves when very still. The wellspring of the sound might be a motor, a robber caution or a bird singing. Whatever it is, some important for it should vibrate for it to deliver sound. Indeed, it is not difficult to distinguish the vibrations of numerous wellsprings of sound. On the off chance that you contact your throat while singing or talking, you can feel the vibrations of your vocal strings. Essentially, an amplifier vibrates unequivocally particularly when the volume is turned up. By gently contacting the speaker cone, you can feel its vibrations as a shivering sensation in your fingertips. For such vibrations to be heard as sound, there should be a medium through which they can go from the vibrating source to the ear. For instance, sound voyages obviously through water, as any swimmer can affirm, and it additionally ventures uncommonly well through metal. The vast majority of the sounds we hear, notwithstanding, are sent through air. The vibrations of a sound source prompt the adjoining air particles to be then again crushed together and pulled separated. These air atoms at that point push and pull against their neighbours which, thusly, push and pull against their neighbours. Therefore, a progression of compressions (locales of higher pressing factor) and rarefactions (areas of lower pressure) is produced which ventures from the vibrating source. This arrangement of pressing factor changes is the thing that we allude to as a sound wave. [1]
Sound is delivered when an item vibrates, making a pressing factor wave. This pressing factor wave causes particles in the encompassing medium (air, water, or strong) to have vibrational movement. As the particles vibrate, they move close by particles, sending the sound further through the medium. The human ear identifies sound waves when vibrating air particles vibrate little parts inside the ear. [2]


Framework & Limitations:


There are five fundamental qualities of sound waves: wavelength, amplitude, frequency, time period, and velocity. The frequency of a sound wave shows the distance that wave goes before it rehashes the same thing. The actual frequency is a longitudinal wave that shows the compressions and rarefactions of the sound wave. The plentifulness of a wave characterizes the greatest dislodging of the particles upset by the sound wave as it goes through a medium. A huge sufficiency shows a huge sound wave. The recurrence of a sound wave demonstrates the quantity of sound waves delivered each second. Low-recurrence sounds produce sound waves less frequently than high-recurrence sounds. The timeframe of a sound wave is the measure of time needed to make a total wave cycle. Every vibration from the sound source delivers a wave of sound. Each total wave cycle starts with a box and finishes toward the beginning of the following box. Ultimately, the speed of a sound wave discloses to us how quick the wave is moving and is communicated as meters per second. [2]
Sound goes at a specific speed just like. One can notice direct proof of the speed of sound while watching a light show. The glimmer of a blast is seen a long time before its sound is heard, inferring both that sound goes at a limited speed and that it is much slower than light. One can likewise straightforwardly detect the recurrence of a sound. Impression of recurrence is called pitch. The frequency of sound is not straightforwardly detected, however circuitous proof is found in the relationship of the size of instruments with their pitch. Little instruments regularly make high-pitch sounds, while enormous instruments ordinarily make low-pitch sounds. High pitch implies little frequency, and the size of an instrument is straightforwardly identified with the frequencies of sound it produces. A little instrument makes short-frequency sounds. Comparative contentions hold that an enormous instrument makes long-frequency sounds. [3]


Literature review:


After creation sound studios add the sound that goes with the image in films. Sound can make the invention of this present reality and dousing and is pivotal portion of the innovative creation. Sounds are in like manner needed in other fluctuating media manifestations, for instance, in PC games or site pages. At times sounds are recorded for the occasion. Usually, some engineers access recently amassed sound eﬀects libraries. In this case a post-production company adds commentary to a pre-recorded football match. What was interesting was to assign the commentary audio to the football match audio in such a way that it sounds all natural and also at the exact timings based on the events happened in the football match. The company also exported the merged audio as one audio file using the user interface which goes hand in hand with the functions of the software. Another engaging part was to align the commentary audio to any point in the match audio in an efficient way.


Related Work & Applications:


Existing audio classiﬁcation strategies are typically ﬁnely tuned to little spaces, like instrument classiﬁcation or simpliﬁed sound eﬀects scientific categorizations. Freely of the element extraction and choice strategy and the sort of classiﬁer, content-based classiﬁcation frameworks need a bunch of classes and an enormous number of sound examples for each class to prepare the framework. [4]
A focal objective of the music data recovery local area is to make frameworks that effectively store and recover melodies from huge data sets of melodic substance. The most well-known approach to store and recover music utilizes metadata, for example, the name of the writer or craftsman, the name of the tune, or the delivery date of the collection. Thinking about a broader meaning of melodic metadata as any non-acoustic portrayal of a tune. This incorporates classification and instrument names, melody audits, appraisals as per bipolar descriptors and buy deals records. These portrayals can be utilized as contribution to cooperative sifting frameworks that help clients look for music. The disadvantage of these frameworks is that they require a novel melody to be physically clarified before it tends to be recovered. [5]
Depicting a soundscape explanation instrument for unconstrained ecological sounds. The instrument sections the time-recurrence plane into locales that are probably going to come from a solitary source. A human annotator can arrange these areas. The framework figures out how to recommend potential comments and presents these for acknowledgment. Acknowledged or rectified explanations were utilized to improve the grouping further. Programmed explanations with a high likelihood of being right were acknowledged naturally. This speeded up the explanation interaction and made it conceivable to clarify complex soundscapes both rapidly and in significant detail. [6]
Huge examination endeavours were devoted to the programmed discovery of unusual lung sounds, utilizing, for this reason, various calculations. The approval of these calculations depends on the examination of their outcomes with reference comments and hence requires the improvement of easy to understand comment programming. For the comment of respiratory sounds. The client can recognize respiratory cycles and extrinsic sounds – pops and wheezes straightforwardly on the waveforms showed on the screen, which might be at the same time played back. The sound playback speed is client flexible and synchronized with the cursor show. Explicit comment document stockpiling designs were characterized. Fundamental ease of use tests performed by three wellbeing experts utilizing twenty respiratory sound records from six patients (with pneumonia and cystic fibrosis) demonstrate that the product was easy to use and successful, permitting basic and speedy explanations. [7]


User Interface and Design:


The first thought put into the design of the sound annotator was for it to be easily navigable and user friendly when it came to adding new soundtracks and exporting the final audio file. The annotator also consisted of the graph to display the signal strength of the imported audio. Two subsequent audio files could be imported so aligning 2 separate graphs were displayed on the right side of the GUI. Each graph has a track controller with buttons for importing, playing, pausing, and stopping the imported audio file. There is a speed monitor tracker so that the user can manage the speed of the playing sound, with reset speed controller and a slider.
One of the controllers has the Merge button to mix the two separate audios with a slider to choose at which instance to merge the respective audios. The other tracker will have the save button to export the merged audio. Both the trackers will have a loop button which has a radio feature so the person can stop or let the sound be on loop. The Signal strength graph was plotted with respect to time for both the audio graphs. Every component in the graphic user interface was well named and easy to recognise for any user and labelled as Sound Annotator.
Sound Annotator GUI
 
 ![image](https://user-images.githubusercontent.com/77658144/117635740-b2621700-b1b2-11eb-8ec7-a781a555f5e2.png)

 
 
The track controllers and speed controller were placed on the left side of the annotator, the graphs were placed on the right side aligned to the respective controllers, they also had a time bar to measure the sounds duration in seconds and every button and tab was labelled.
Track 1 Controller


![image](https://user-images.githubusercontent.com/77658144/117635765-ba21bb80-b1b2-11eb-8fe1-0a901d7cea02.png)

 
Controller one has six buttons out of which the loop button is a radio button to turn on and off the looping of the sound. The select file button helps the user to choose any audio file from the system and browse folders and select the desired file. The play button on click starts to play the audio. The pause button will halt the playing audio at the exact time while the stop button will not just halt the playing audio but also bring back the time bar to zero so the audio will play from the start again, finally the save button will retain the desired audio and the user can save the file in the system with any name.
Track 2 Controller


![image](https://user-images.githubusercontent.com/77658144/117635791-c1e16000-b1b2-11eb-9cda-23e6954aa3aa.png)


 
Second one had all similar features as the first controller except the start button. It has a merge at slider to allow the user to merge the audio at a specific time and the merge button to finally merge it together and display it on the first audio graph.
Track Speed
 
 
 ![image](https://user-images.githubusercontent.com/77658144/117635820-c9a10480-b1b2-11eb-8598-f1ba3d8c320e.png)


User can manipulate the speed of the playing audio using the slider in the track speed controller as well as reset it back to 1 using the reset button on the right side of the slider.

Functions and Working
Importing Audio Files
As soon as the user clicks on the select file button on either of the trackers the system opens up the file explorer for the user to select the audio from the computer to import in the annotator tool.
 
 ![image](https://user-images.githubusercontent.com/77658144/117635849-d160a900-b1b2-11eb-89ed-7ee806ec588f.png)


In the background the following code runs, and the uses the object and the variable assigned to the select button to execute the code and function as programmed, which is to let the user select an audio file.
 
 
Merging Audio Files
 
 ![image](https://user-images.githubusercontent.com/77658144/117635892-df162e80-b1b2-11eb-9ff2-50e1ed7b7048.png)


The following line of code is executed when the user clicks on the merge button to merge the two audio files selected on the sound annotator.
 
 
The merge at slider holds the time in the annotator so when the user selects a specific time to merge the audio the annotator merges the files at the exact second.
 
 ![image](https://user-images.githubusercontent.com/77658144/117635938-e89f9680-b1b2-11eb-8497-f351e31d6567.png)


The following code that runs behind this set up takes the variables assigned to the merge at slider and adds to the final merged audio.
 
Export Resulting Audio
 
 ![image](https://user-images.githubusercontent.com/77658144/117635990-f1906800-b1b2-11eb-942e-28316067dcb0.png)


The user can exported the merged audio using the save button which is on the first audio tracker and the system will prompt the user in which folder and with what name the user would like to save the file in the system.
 
 
The save button function in the background will call back the object assigned to the button and save the track in the desired location.
Additional Features
Play Button
On selecting any audio file from the system, the user is able to play the sound in the sound annotator using the play button. The call back function that goes behind it is as follows:
 
 

Pause Button
When the audio file is playing the user can pause the audio at the exact moment using the pause button, the function pause button is as follows
 
 

Stop Button
If the user decides to stop the audio file and bring the slider back to zero that is the start of the audio, they can click on the stop button and the stop button function will be executed:
 
 

Loop Button
A User an also play the audio file on loop using the radio button labelled as loop and clicking on it, the call back function is executed when the checkbox is marked  :
 
Audio Graph
As soon as the user selects an audio file the graph aligned to the tracker shows the frequency of the audio’s signal strength with respect to time.
 
 
 ![image](https://user-images.githubusercontent.com/77658144/117636027-fb19d000-b1b2-11eb-9a7b-41a5a4c01291.png)


The specific function displays the axes for the audio file selected.
 
 

Key Features
One of the most interesting features of the sound annotator was to merge the sound at a specific moment when the user decides to mix the sound files, the implementation takes place by adding the merge button in the GUI in the second audio tracker. The variable given to the function was ‘fn_combine_tracks’ and the handle name was passed to the function. It also had the variables to access the loaded tracks and the merge at time. The function involved multiple if and else statements with multiple conditions like if the track has a single channel the channel is duplicated and so on.
 

Once the tracks were selected it took account of the sample rate and executed the calculations for the merging of the tracks for the resulting audio. One thing that was also considered was that if any of the audio track is shorter compared to the other one, an empty track is added to make the track equal.

 

Finally, the two tracks are combined and the first audio tracker with the graph is updated with the merged audio with the timer and stop functions and with the user interface.
 

And as soon as the user selects the preferred file destination and name the audio file is saved in the system.
Conclusion
Sound Production teams can run into many challenges while adding sounds to a pre recorded audio as there is not enough data required for the desired output. The Sound Annotator helps to perform the task to merge two separate audios, but it can always vary depending on the user. Features like sound effects and more functionalities can be added to the annotator, with new technologies emerging it was also important to integrate several different functions to put them together to find the best possible solution.
References

[1] 	“OpenLearn,” 2019. [Online]. Available: https://www.open.edu/openlearn/science-maths-technology/science/physics-and-astronomy/physics/what-sound.
[2] 	“Pasco,” [Online]. Available: https://www.pasco.com/products/guides/sound-waves.
[3] 	“Speed of Sound, Frequency, and Wavelength,” [Online]. Available: https://courses.lumenlearning.com/physics/chapter/17-2-speed-of-sound-frequency-and-wavelength/.
[4] 	P. C. a. M. Koppenberger, “AUTOMATIC SOUND ANNOTATION,” 2004. 
[5] 	L. B. D. T. a. G. L. Douglas Turnbull, “Semantic Annotation and Retrieval of Music and Sound Effects,” IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, vol. 16, no. 2, 2008. 
[6] 	M. E. N. T. C. A. Dirkjan Krijnders, “Annotating Soundscapes,” 2009. 
[7] 	G. C. J. R. A. M. João Dinis, “RESPIRATORY SOUND ANNOTATION SOFTWARE”. 
[8] 	“Github,” [Online]. Available: https://github.com/sandeshbindukar/SoundAnnotator.



